{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from this link:\n",
    "\n",
    "https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description about dataset::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. \n",
    "\n",
    "\n",
    "### Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKFLOW :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Load Data\n",
    "\n",
    "2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )\n",
    "\n",
    "3.Standardized the Input Variables. \n",
    "\n",
    "4.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).\n",
    "\n",
    "5.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).\n",
    "\n",
    "6.Compilation Step (Note : Its a Binary problem , select loss , metrics according to it)\n",
    "\n",
    "7.Train the Model with Epochs (100).\n",
    "\n",
    "8.If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .\n",
    "\n",
    "9.Prediction should be > 92%\n",
    "10.Evaluation Step\n",
    "11Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify fraudulent credit card transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models,layers\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./creditcard.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fraud=df[df[\"Class\"]==0]\n",
    "fraud = df[df[\"Class\"]==1]\n",
    "non_fraud=non_fraud.sample(3*fraud.shape[0])\n",
    "data = fraud.append(non_fraud, ignore_index=True)\n",
    "# data[\"Class\"].value_counts()\n",
    "x_data= data.drop(columns=\"Class\", axis=0)\n",
    "label = data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels=train_test_split(x_data, label, test_size=0.3, random_state=1, stratify = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_data.mean(axis=0) # taking the mean of \n",
    "train_data -= train_mean\n",
    "train_std = train_data.std(axis=0)\n",
    "train_data /= train_std\n",
    "test_data -= train_mean\n",
    "test_data /= train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>1.479278</td>\n",
       "      <td>0.790737</td>\n",
       "      <td>-0.519405</td>\n",
       "      <td>0.062660</td>\n",
       "      <td>-0.729994</td>\n",
       "      <td>0.121302</td>\n",
       "      <td>-0.735743</td>\n",
       "      <td>0.327838</td>\n",
       "      <td>-0.216164</td>\n",
       "      <td>-0.132621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106604</td>\n",
       "      <td>-0.071658</td>\n",
       "      <td>0.220988</td>\n",
       "      <td>0.181418</td>\n",
       "      <td>0.100259</td>\n",
       "      <td>-0.046038</td>\n",
       "      <td>-0.701557</td>\n",
       "      <td>-0.125144</td>\n",
       "      <td>-0.134283</td>\n",
       "      <td>-0.174668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>-0.169031</td>\n",
       "      <td>0.084833</td>\n",
       "      <td>0.855763</td>\n",
       "      <td>-0.959624</td>\n",
       "      <td>1.379723</td>\n",
       "      <td>-0.452715</td>\n",
       "      <td>-1.246308</td>\n",
       "      <td>-1.005621</td>\n",
       "      <td>0.336598</td>\n",
       "      <td>-1.954056</td>\n",
       "      <td>...</td>\n",
       "      <td>1.115828</td>\n",
       "      <td>0.401005</td>\n",
       "      <td>-0.499256</td>\n",
       "      <td>-0.257300</td>\n",
       "      <td>-0.608236</td>\n",
       "      <td>1.532433</td>\n",
       "      <td>2.433244</td>\n",
       "      <td>1.586640</td>\n",
       "      <td>1.717480</td>\n",
       "      <td>-0.432897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>-0.016866</td>\n",
       "      <td>0.745137</td>\n",
       "      <td>0.236788</td>\n",
       "      <td>-0.528218</td>\n",
       "      <td>0.453924</td>\n",
       "      <td>0.892547</td>\n",
       "      <td>-0.799513</td>\n",
       "      <td>0.470865</td>\n",
       "      <td>-0.110370</td>\n",
       "      <td>0.995623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262831</td>\n",
       "      <td>-0.343130</td>\n",
       "      <td>-0.967257</td>\n",
       "      <td>-0.005484</td>\n",
       "      <td>0.037020</td>\n",
       "      <td>0.612384</td>\n",
       "      <td>-1.225780</td>\n",
       "      <td>-0.037371</td>\n",
       "      <td>0.183483</td>\n",
       "      <td>-0.428286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>-1.170784</td>\n",
       "      <td>0.068908</td>\n",
       "      <td>-0.010341</td>\n",
       "      <td>0.583101</td>\n",
       "      <td>0.087551</td>\n",
       "      <td>0.188865</td>\n",
       "      <td>0.231424</td>\n",
       "      <td>0.442799</td>\n",
       "      <td>0.036334</td>\n",
       "      <td>0.306749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065826</td>\n",
       "      <td>-0.145868</td>\n",
       "      <td>0.265251</td>\n",
       "      <td>0.160837</td>\n",
       "      <td>0.184927</td>\n",
       "      <td>-0.134201</td>\n",
       "      <td>-0.545418</td>\n",
       "      <td>-0.001546</td>\n",
       "      <td>-0.308285</td>\n",
       "      <td>-0.168766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>0.513072</td>\n",
       "      <td>0.411285</td>\n",
       "      <td>-0.227943</td>\n",
       "      <td>0.451921</td>\n",
       "      <td>-0.014194</td>\n",
       "      <td>-0.070914</td>\n",
       "      <td>0.745837</td>\n",
       "      <td>0.308934</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>0.551685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015356</td>\n",
       "      <td>0.019876</td>\n",
       "      <td>1.110197</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>-0.738152</td>\n",
       "      <td>-1.310357</td>\n",
       "      <td>2.048378</td>\n",
       "      <td>-0.262108</td>\n",
       "      <td>-0.255144</td>\n",
       "      <td>0.099470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>-1.445346</td>\n",
       "      <td>0.578215</td>\n",
       "      <td>-0.427996</td>\n",
       "      <td>0.520287</td>\n",
       "      <td>-0.657729</td>\n",
       "      <td>0.020820</td>\n",
       "      <td>0.201683</td>\n",
       "      <td>0.145615</td>\n",
       "      <td>-0.036463</td>\n",
       "      <td>2.023808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197021</td>\n",
       "      <td>-0.193600</td>\n",
       "      <td>0.072834</td>\n",
       "      <td>-0.025312</td>\n",
       "      <td>-0.771765</td>\n",
       "      <td>0.793909</td>\n",
       "      <td>-1.420396</td>\n",
       "      <td>0.028331</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>-0.378254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>0.636217</td>\n",
       "      <td>0.752836</td>\n",
       "      <td>-0.219208</td>\n",
       "      <td>-0.035949</td>\n",
       "      <td>-0.020752</td>\n",
       "      <td>0.499821</td>\n",
       "      <td>-0.354785</td>\n",
       "      <td>0.502363</td>\n",
       "      <td>-0.168888</td>\n",
       "      <td>0.079198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199389</td>\n",
       "      <td>-0.025878</td>\n",
       "      <td>0.647426</td>\n",
       "      <td>-0.047213</td>\n",
       "      <td>-0.554945</td>\n",
       "      <td>0.822513</td>\n",
       "      <td>-1.042556</td>\n",
       "      <td>-0.108689</td>\n",
       "      <td>-0.178451</td>\n",
       "      <td>-0.283032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>-0.620890</td>\n",
       "      <td>-0.921096</td>\n",
       "      <td>-0.410212</td>\n",
       "      <td>-0.425563</td>\n",
       "      <td>1.526487</td>\n",
       "      <td>-2.116270</td>\n",
       "      <td>2.646582</td>\n",
       "      <td>0.618735</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-1.251858</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.612451</td>\n",
       "      <td>0.019257</td>\n",
       "      <td>1.426299</td>\n",
       "      <td>-0.902826</td>\n",
       "      <td>-0.043714</td>\n",
       "      <td>-0.566902</td>\n",
       "      <td>0.906120</td>\n",
       "      <td>1.762561</td>\n",
       "      <td>-2.652046</td>\n",
       "      <td>6.032798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.054394</td>\n",
       "      <td>0.491116</td>\n",
       "      <td>-0.437148</td>\n",
       "      <td>-0.144859</td>\n",
       "      <td>1.339371</td>\n",
       "      <td>0.478558</td>\n",
       "      <td>-0.187718</td>\n",
       "      <td>0.636965</td>\n",
       "      <td>-0.207650</td>\n",
       "      <td>0.282712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928701</td>\n",
       "      <td>-0.167446</td>\n",
       "      <td>-1.259831</td>\n",
       "      <td>-0.261470</td>\n",
       "      <td>-0.632135</td>\n",
       "      <td>-0.370313</td>\n",
       "      <td>-0.589770</td>\n",
       "      <td>-0.242289</td>\n",
       "      <td>0.236944</td>\n",
       "      <td>2.202334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-1.179702</td>\n",
       "      <td>-0.161959</td>\n",
       "      <td>0.189918</td>\n",
       "      <td>0.360507</td>\n",
       "      <td>-0.112001</td>\n",
       "      <td>0.539749</td>\n",
       "      <td>-0.203175</td>\n",
       "      <td>0.092329</td>\n",
       "      <td>-0.972382</td>\n",
       "      <td>0.345644</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.073020</td>\n",
       "      <td>1.257199</td>\n",
       "      <td>-1.185072</td>\n",
       "      <td>-0.093194</td>\n",
       "      <td>-0.068593</td>\n",
       "      <td>-0.555489</td>\n",
       "      <td>1.080079</td>\n",
       "      <td>0.685812</td>\n",
       "      <td>0.350514</td>\n",
       "      <td>-0.429393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>591 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time        V1        V2        V3        V4        V5        V6  \\\n",
       "1370  1.479278  0.790737 -0.519405  0.062660 -0.729994  0.121302 -0.735743   \n",
       "265  -0.169031  0.084833  0.855763 -0.959624  1.379723 -0.452715 -1.246308   \n",
       "289  -0.016866  0.745137  0.236788 -0.528218  0.453924  0.892547 -0.799513   \n",
       "644  -1.170784  0.068908 -0.010341  0.583101  0.087551  0.188865  0.231424   \n",
       "1818  0.513072  0.411285 -0.227943  0.451921 -0.014194 -0.070914  0.745837   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1007 -1.445346  0.578215 -0.427996  0.520287 -0.657729  0.020820  0.201683   \n",
       "1822  0.636217  0.752836 -0.219208 -0.035949 -0.020752  0.499821 -0.354785   \n",
       "210  -0.620890 -0.921096 -0.410212 -0.425563  1.526487 -2.116270  2.646582   \n",
       "322   0.054394  0.491116 -0.437148 -0.144859  1.339371  0.478558 -0.187718   \n",
       "93   -1.179702 -0.161959  0.189918  0.360507 -0.112001  0.539749 -0.203175   \n",
       "\n",
       "            V7        V8        V9  ...       V20       V21       V22  \\\n",
       "1370  0.327838 -0.216164 -0.132621  ...  0.106604 -0.071658  0.220988   \n",
       "265  -1.005621  0.336598 -1.954056  ...  1.115828  0.401005 -0.499256   \n",
       "289   0.470865 -0.110370  0.995623  ... -0.262831 -0.343130 -0.967257   \n",
       "644   0.442799  0.036334  0.306749  ... -0.065826 -0.145868  0.265251   \n",
       "1818  0.308934 -0.022095  0.551685  ...  0.015356  0.019876  1.110197   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1007  0.145615 -0.036463  2.023808  ... -0.197021 -0.193600  0.072834   \n",
       "1822  0.502363 -0.168888  0.079198  ... -0.199389 -0.025878  0.647426   \n",
       "210   0.618735  0.004787 -1.251858  ... -1.612451  0.019257  1.426299   \n",
       "322   0.636965 -0.207650  0.282712  ...  0.928701 -0.167446 -1.259831   \n",
       "93    0.092329 -0.972382  0.345644  ... -1.073020  1.257199 -1.185072   \n",
       "\n",
       "           V23       V24       V25       V26       V27       V28    Amount  \n",
       "1370  0.181418  0.100259 -0.046038 -0.701557 -0.125144 -0.134283 -0.174668  \n",
       "265  -0.257300 -0.608236  1.532433  2.433244  1.586640  1.717480 -0.432897  \n",
       "289  -0.005484  0.037020  0.612384 -1.225780 -0.037371  0.183483 -0.428286  \n",
       "644   0.160837  0.184927 -0.134201 -0.545418 -0.001546 -0.308285 -0.168766  \n",
       "1818  0.015618 -0.738152 -1.310357  2.048378 -0.262108 -0.255144  0.099470  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1007 -0.025312 -0.771765  0.793909 -1.420396  0.028331  0.041162 -0.378254  \n",
       "1822 -0.047213 -0.554945  0.822513 -1.042556 -0.108689 -0.178451 -0.283032  \n",
       "210  -0.902826 -0.043714 -0.566902  0.906120  1.762561 -2.652046  6.032798  \n",
       "322  -0.261470 -0.632135 -0.370313 -0.589770 -0.242289  0.236944  2.202334  \n",
       "93   -0.093194 -0.068593 -0.555489  1.080079  0.685812  0.350514 -0.429393  \n",
       "\n",
       "[591 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu',kernel_regularizer=regularizers.l2(0.001), input_shape=(train_data.shape[1],)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(8, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(6, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'binary_crossentropy', optimizer= 'rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 32ms/step - loss: 0.8316 - accuracy: 0.5881 - val_loss: 0.7031 - val_accuracy: 0.7391\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.7815 - accuracy: 0.6513 - val_loss: 0.6817 - val_accuracy: 0.7391\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.7600 - accuracy: 0.7055 - val_loss: 0.6706 - val_accuracy: 0.7391\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.7275 - accuracy: 0.6860 - val_loss: 0.6529 - val_accuracy: 0.7391\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.7118 - accuracy: 0.7033 - val_loss: 0.6320 - val_accuracy: 0.7391\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6640 - accuracy: 0.7243 - val_loss: 0.6085 - val_accuracy: 0.7391\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.7394 - val_loss: 0.5928 - val_accuracy: 0.7391\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6505 - accuracy: 0.7402 - val_loss: 0.5766 - val_accuracy: 0.7391\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6155 - accuracy: 0.7519 - val_loss: 0.5555 - val_accuracy: 0.7391\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5976 - accuracy: 0.7894 - val_loss: 0.5341 - val_accuracy: 0.7415\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.7744 - val_loss: 0.5081 - val_accuracy: 0.7560\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.7778 - val_loss: 0.4773 - val_accuracy: 0.7633\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5431 - accuracy: 0.7753 - val_loss: 0.4428 - val_accuracy: 0.8019\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.7695 - val_loss: 0.4176 - val_accuracy: 0.8406\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5308 - accuracy: 0.7975 - val_loss: 0.3882 - val_accuracy: 0.8575\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5366 - accuracy: 0.7871 - val_loss: 0.3655 - val_accuracy: 0.8647\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.8095 - val_loss: 0.3388 - val_accuracy: 0.8816\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7851 - val_loss: 0.3179 - val_accuracy: 0.9082\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.8212 - val_loss: 0.3063 - val_accuracy: 0.9155\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.8142 - val_loss: 0.2917 - val_accuracy: 0.9275\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.8015 - val_loss: 0.2781 - val_accuracy: 0.9348\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.8227 - val_loss: 0.2623 - val_accuracy: 0.9372\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8426 - val_loss: 0.2516 - val_accuracy: 0.9396\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3853 - accuracy: 0.8567 - val_loss: 0.2383 - val_accuracy: 0.9420\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4450 - accuracy: 0.8430 - val_loss: 0.2277 - val_accuracy: 0.9469\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.5087 - accuracy: 0.8486 - val_loss: 0.2218 - val_accuracy: 0.9469\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 0.3535 - accuracy: 0.8857 - val_loss: 0.2131 - val_accuracy: 0.9493\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3850 - accuracy: 0.8551 - val_loss: 0.2064 - val_accuracy: 0.9493\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.8495 - val_loss: 0.2015 - val_accuracy: 0.9469\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3866 - accuracy: 0.8667 - val_loss: 0.1962 - val_accuracy: 0.9517\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8882 - val_loss: 0.1939 - val_accuracy: 0.9517\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8769 - val_loss: 0.1895 - val_accuracy: 0.9541\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8975 - val_loss: 0.1901 - val_accuracy: 0.9517\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8778 - val_loss: 0.1878 - val_accuracy: 0.9541\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.8834 - val_loss: 0.1865 - val_accuracy: 0.9565\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.8945 - val_loss: 0.1840 - val_accuracy: 0.9565\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.9055 - val_loss: 0.1845 - val_accuracy: 0.9565\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.3475 - accuracy: 0.8980 - val_loss: 0.1834 - val_accuracy: 0.9565\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8894 - val_loss: 0.1807 - val_accuracy: 0.9565\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.9057 - val_loss: 0.1820 - val_accuracy: 0.9565\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8939 - val_loss: 0.1803 - val_accuracy: 0.9565\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.3054 - accuracy: 0.9082 - val_loss: 0.1778 - val_accuracy: 0.9565\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.9054 - val_loss: 0.1767 - val_accuracy: 0.9565\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3158 - accuracy: 0.9135 - val_loss: 0.1755 - val_accuracy: 0.9565\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.9164 - val_loss: 0.1737 - val_accuracy: 0.9565\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.3017 - accuracy: 0.9093 - val_loss: 0.1733 - val_accuracy: 0.9565\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.9163 - val_loss: 0.1717 - val_accuracy: 0.9565\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.9059 - val_loss: 0.1723 - val_accuracy: 0.9565\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8896 - val_loss: 0.1723 - val_accuracy: 0.9565\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.2780 - accuracy: 0.8991 - val_loss: 0.1762 - val_accuracy: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e0001748b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, epochs=50,validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 889us/step - loss: 0.2200 - accuracy: 0.9543\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22000302374362946"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543147087097168"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (predictions > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = tf.math.confusion_matrix(\n",
    "    test_labels, y_pred, num_classes=2, weights=None, dtype=tf.dtypes.int32,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[440,   3],\n",
       "       [ 24, 124]])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.43147087097168"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = accuracy * 100\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
